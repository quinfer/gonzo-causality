
@article{Cunningham.2021, 
year = {2021}, 
title = {{Causal Inference}}, 
author = {Cunningham, Scott}, 
doi = {10.12987/9780300255881}, 
keywords = {}
}
@book{Greene.2019, 
year = {2019}, 
title = {{Econometric Analysis}}, 
author = {Greene, William H}, 
editor = {Pearson}, 
isbn = {9780134461366}, 
url = {https://www.pearson.com/en-gb/subject-catalog/p/econometric-analysis-global-edition/P200000004340/9781292231150}, 
keywords = {}, 
edition = {8th}, 
month = {9}
}

@article{Leontief.1982, 
year = {1982}, 
title = {{Academic Economics}}, 
author = {Leontief, Wassily}, 
journal = {Science}, 
issn = {0036-8075}, 
doi = {10.1126/science.217.4555.104}, 
pmid = {17770240}, 
pages = {104--107}, 
number = {4555}, 
volume = {217}, 
keywords = {}, 
local-url = {file://localhost/var/folders/yd/rdwvt_yn6xv6m26pmptqyh6r0000gp/T/science.217.4555.104.pdf}
}

@article{Prado.2022, 
year = {2022}, 
title = {{Machine Learning for Econometricians: The Readme Manual}}, 
author = {Prado, Marcos López de}, 
journal = {The Journal of Financial Data Science}, 
issn = {2640-3943}, 
doi = {10.3905/jfds.2022.1.101}, 
pages = {10--30}, 
number = {3}, 
volume = {4}, 
keywords = {}, 
local-url = {file://localhost/Users/quinference/Documents/Papers%20Library/Prado-Machine%20Learning%20for%20Econometricians-%20The%20Readme%20Manual-2022-The%20Journal%20of%20Financial%20Data%20Science.pdf}
}

@article{Nogueira.2022, 
year = {2022}, 
title = {{Methods and tools for causal discovery and causal inference}}, 
author = {Nogueira, Ana Rita and Pugnana, Andrea and Ruggieri, Salvatore and Pedreschi, Dino and Gama, João}, 
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery}, 
issn = {1942-4787}, 
doi = {10.1002/widm.1449}, 
abstract = {{Causality is a complex concept, which roots its developments across several fields, such as statistics, economics, epidemiology, computer science, and philosophy. In recent years, the study of causal relationships has become a crucial part of the Artificial Intelligence community, as causality can be a key tool for overcoming some limitations of correlation‐based Machine Learning systems. Causality research can generally be divided into two main branches, that is, causal discovery and causal inference. The former focuses on obtaining causal knowledge directly from observational data. The latter aims to estimate the impact deriving from a change of a certain variable over an outcome of interest. This article aims at covering several methodologies that have been developed for both tasks. This survey does not only focus on theoretical aspects. But also provides a practical toolkit for interested researchers and practitioners, including software, datasets, and running examples. This article is categorized under: Algorithmic Development > Causality Discovery Fundamental Concepts of Data and Knowledge > Explainable AI Technologies > Machine Learning In recent years, the study of causal relationships has become a crucial part of the AI community, as causality can be a key tool for overcoming some limitations of correlation‐based machine learning systems. This survey does not only focus on theoretical aspects. It also provides a practical toolkit for interested researchers and practitioners, including software, datasets, and running examples.}}, 
number = {2}, 
volume = {12}, 
keywords = {}
}
@TechReport{Forney.2021,
  author = 	 "Forney, A. and Mueller, S.",
  title = 	 "Causal Inference in AI Education: A Primer",
  institution =  "Department of Computer Science, 
 		  University of California, Los Angeles",
  address = 	 "CA",
  year = 	 2021,
  number =	 "R-509, {$<$http://ftp.cs.ucla.edu/pub/stat\_ser/r509.pdf$>$}",
  note =	 "Forthcoming, {\em Journal of Causal Inference}"
}

@book{Lee.2018,
author = {Lee, Kai-Fu},
title = {AI Superpowers: China, Silicon Valley, and the New World Order},
year = {2018},
isbn = {132854639X},
publisher = {Houghton Mifflin Co.},
address = {USA},
abstract = {THE NEW YORK TIMES, USA TODAY, AND WALL STREET JOURNAL BESTSELLERDr. Kai-Fu Leeone of the worlds most respected experts on AI and Chinareveals that China has suddenly caught up to the US at an astonishingly rapid and unexpected pace. In AI Superpowers, Kai-fu Lee argues powerfully that because of these unprecedented developments in AI, dramatic changes will be happening much sooner than many of us expected. Indeed, as the US-Sino AI competition begins to heat up, Lee urges the US and China to both accept and to embrace the great responsibilities that come with significant technological power. Most experts already say that AI will have a devastating impact on blue-collar jobs. But Lee predicts that Chinese and American AI will have a strong impact on white-collar jobs as well. Is universal basic income the solution? In Lees opinion, probably not. But he provides a clear description of which jobs will be affected and how soon, which jobs can be enhanced with AI, and most importantly, how we can provide solutions to some of the most profound changes in human history that are coming soon.}
}

@article{Fisher.1935, 
year = {1935}, 
title = {{The Design of Experiments.}}, 
author = {Hotelling, Harold and Fisher, R A}, 
journal = {Journal of the American Statistical Association}, 
issn = {0162-1459}, 
doi = {10.2307/2277749}, 
pages = {771}, 
number = {192}, 
volume = {30}, 
keywords = {}
}

@article{Neyman.1935, 
year = {1935}, 
title = {{Statistical Problems in Agricultural Experimentation}}, 
author = {Neyman, J. and Iwaszkiewicz, K. and Kołodziejczyk, St.}, 
journal = {Supplement to the Journal of the Royal Statistical Society}, 
issn = {1466-6162}, 
doi = {10.2307/2983637}, 
pages = {107--154}, 
number = {2}, 
volume = {2}, 
keywords = {}
}
@book{Pearl.2009, 
keywords = {causality}, 
title = {{Causality}}, 
author = {Judea, Pearl,}, 
isbn = {9780521895606}, 
url = {http://bayes.cs.ucla.edu/BOOK-2K/neuberg-review.pdf}, 
abstract = {{Written by one of the preeminent researchers in the field, this book      provides a comprehensive exposition of modern analysis of causation. It      shows how causality has grown from a nebulous concept into a mathematical      theory with significant applications in the fields of statistics,      artificial intelligence, economics, philosophy, cognitive science, and the      health and social sciences. Judea Pearl presents and unifies the      probabilistic, manipulative, counterfactual, and structural approaches to      causation and devises simple mathematical tools for studying the      relationships between causal connections and statistical associations. The      book will open the way for including causal analysis in the standard      curricula of statistics, artificial intelligence, business, epidemiology,      social sciences, and economics. Students in these fields will find natural      models, simple inferential procedures, and precise mathematical      definitions of causal concepts that traditional texts have evaded or made      unduly complicated. The first edition of Causality has led to a      paradigmatic change in the way that causality is treated in statistics,      philosophy, computer science, social science, and economics. Cited in more      than 5,000 scientific publications, it continues to liberate scientists      from the traditional molds of statistical thinking. In this revised      edition, Judea Pearl elucidates thorny issues, answers readers' questions,      and offers a panoramic view of recent advances in this field of research.      Causality will be of interests to students and professionals in a wide      variety of fields. Anyone who wishes to elucidate meaningful relationships      from data, predict effects of actions and policies, assess explanations of      reported events, or form theories of causal understanding and causal      speech will find this book stimulating and invaluable.}}, 
publisher = {Cambridge University Press}, 
year = {2009}, 
month = {9}
}

@article{Heckman.2022, 
year = {2022}, 
title = {{Causality and Econometrics}}, 
author = {Heckman, James J. and Pinto, Rodrigo}, 
journal = {SSRN Electronic Journal}, 
doi = {10.2139/ssrn.4048252}, 
abstract = {{This paper examines the econometric causal model for policy analysis developed by the seminal ideas of Ragnar Frisch and Trygve Haavelmo. We compare the econometric causal model with two popular causal frameworks: Neyman-Holland causal model and the do-calculus. The Neyman-Holland causal model is based on the language of potential outcomes and was largely developed by statisticians. The do-calculus, developed by Judea Pearl and co-authors, relies on Directed Acyclic Graphs (DAGs) and is a popular causal framework in computer science. We make the case that economists who uncritically use these approximating frameworks often discard the substantial benefits of the econometric causal model to the detriment of more informative economic policy analyses. We illustrate the versatility and capabilities of the econometric framework using causal models that are frequently studied by economists.}}, 
keywords = {}, 
local-url = {file://localhost/Users/quinference/Dropbox/Mac%20(2)/Documents/Papers%20Library/https::docs.iza.org:dp15081.pdf}
}

@article{Lawrence.2021, 
year = {2021}, 
title = {{Data Generating Process to Evaluate Causal Discovery Techniques for Time Series Data}}, 
author = {Lawrence, Andrew R and Kaiser, Marcus and Sampaio, Rui and Sipos, Maksim}, 
journal = {arXiv}, 
doi = {10.48550/arxiv.2104.08043}, 
eprint = {2104.08043}, 
abstract = {{Going beyond correlations, the understanding and identification of causal relationships in observational time series, an important subfield of Causal Discovery, poses a major challenge. The lack of access to a well-defined ground truth for real-world data creates the need to rely on synthetic data for the evaluation of these methods. Existing benchmarks are limited in their scope, as they either are restricted to a "static" selection of data sets, or do not allow for a granular assessment of the methods' performance when commonly made assumptions are violated. We propose a flexible and simple to use framework for generating time series data, which is aimed at developing, evaluating, and benchmarking time series causal discovery methods. In particular, the framework can be used to fine tune novel methods on vast amounts of data, without "overfitting" them to a benchmark, but rather so they perform well in real-world use cases. Using our framework, we evaluate prominent time series causal discovery methods and demonstrate a notable degradation in performance when their assumptions are invalidated and their sensitivity to choice of hyperparameters. Finally, we propose future research directions and how our framework can support both researchers and practitioners.}}, 
keywords = {}, 
local-url = {file://localhost/Users/quinference/Documents/Papers%20Library/Unknown.pdf}
}
@article{Islam.2022, 
year = {2022}, 
title = {{A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains and Tasks}}, 
author = {Islam, Mir Riyanul and Ahmed, Mobyen Uddin and Barua, Shaibal and Begum, Shahina}, 
journal = {Applied Sciences}, 
doi = {10.3390/app12031353}, 
abstract = {{Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers’ understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.}}, 
pages = {1353}, 
number = {3}, 
volume = {12}, 
keywords = {}
}

@article{Misheva.2021, 
year = {2021}, 
month = {3}, 
title = {{Explainable AI in Credit Risk Management}}, 
author = {Misheva, Branka Hadji and Osterrieder, Joerg and Hirsa, Ali and Kulkarni, Onkar and Lin, Stephen Fung}, 
journal = {arXiv}, 
doi = {10.48550/arXiv.2103.00949}, 
eprint = {2103.00949}, 
abstract = {{Artificial Intelligence (AI) has created the single biggest technology revolution the world has ever seen. For the finance sector, it provides great opportunities to enhance customer experience, democratize financial services, ensure consumer protection and significantly improve risk management. While it is easier than ever to run state-of-the-art machine learning models, designing and implementing systems that support real-world finance applications have been challenging. In large part because they lack transparency and explainability which are important factors in establishing reliable technology and the research on this topic with a specific focus on applications in credit risk management. In this paper, we implement two advanced post-hoc model agnostic explainability techniques called Local Interpretable Model Agnostic Explanations (LIME) and SHapley Additive exPlanations (SHAP) to machine learning (ML)-based credit scoring models applied to the open-access data set offered by the US-based P2P Lending Platform, Lending Club. Specifically, we use LIME to explain instances locally and SHAP to get both local and global explanations. We discuss the results in detail and present multiple comparison scenarios by using various kernels available for explaining graphs generated using SHAP values. We also discuss the practical challenges associated with the implementation of these state-of-art eXplainabale AI (XAI) methods and document them for future reference. We have made an effort to document every technical aspect of this research, while at the same time providing a general summary of the conclusions.}}
}
@article{Prado.2022, 
year = {2022}, 
title = {{Machine Learning for Econometricians: The Readme Manual}}, 
author = {Prado, Marcos López de}, 
journal = {The Journal of Financial Data Science}, 
issn = {2640-3943}, 
doi = {10.3905/jfds.2022.1.101}, 
pages = {10--30}, 
number = {3}, 
volume = {4}, 
keywords = {}, 
local-url = {file://localhost/Users/barry/Documents/Papers%20Library/Prado-Machine%20Learning%20for%20Econometricians-%20The%20Readme%20Manual-2022-The%20Journal%20of%20Financial%20Data%20Science.pdf}
}
}
@article{Apley.2020, 
year = {2020}, 
keywords = {FML_reading,MachineLearning,XAI}, 
title = {{Visualizing the effects of predictor variables in black box supervised      learning models}}, 
author = {Apley, Daniel and Jingyu, Zhu}, 
journal = {J. R. Stat. Soc. Series B Stat. Methodol.}, 
issn = {1369-7412}, 
doi = {10.1111/rssb.12377}, 
url = {http://dx.doi.org/10.1111/rssb.12377}, 
abstract = {{In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel‐weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.}}, 
pages = {1059--1086}, 
number = {4}, 
volume = {82}
}
@article{Owen.2014, 
year = {2014}, 
title = {{Sobol' Indices and Shapley Value}}, 
author = {Owen, Art B.}, 
journal = {SIAM/ASA Journal on Uncertainty Quantification}, 
doi = {10.1137/130936233}, 
abstract = {{Global sensitivity analysis measures the importance of some input variables to a function \$f\$ by looking at the impact on \$f\$ of making large random perturbations to subsets of those variables. Using measures like those of Sobol' we can attribute importance to input variables based on the extent to which they help predict the target function \$f\$. There is a longstanding literature in economics and game theory that considers how to attribute the value of a team effort to individual members of that team. The primary result, known as the Shapley value, is the unique method satisfying some intuitively necessary criteria. In this paper we find the Shapley value of individual variables when we take variance explained as their combined value. The result does not match either of the usual Sobol' indices. It is instead bracketed between them for variance explained or indeed any totally monotone game. Because those indices are comparatively easy to compute, Sobol' indices provide effectively computable bounds for the Shapley value.}}, 
pages = {245--251}, 
number = {1}, 
volume = {2}, 
keywords = {}
}

@inproceedings{Solo.2008,
  title={On causality and mutual information},
  author={Solo, Victor},
  booktitle={2008 47th IEEE Conference on Decision and Control},
  pages={4939--4944},
  year={2008},
  organization={IEEE}
}
@article{Shapley.1988, 
year = {1988}, 
month = {1}, 
title = {{The Shapley value}}, 
author = {Shapley, Lloyd S.}, 
doi = {10.1017/CBO9780511528446.003}, 
abstract = {{Introduction At the foundation of the theory of games is the assumption that the players of a game can evaluate, in their utility scales, every “prospect” that might arise as a result of a play. In attempting to apply the theory to any field, one would normally expect to be permitted to include, in the class of “prospects,” the prospect of having to play a game. The possibility of evaluating games is therefore of critical importance. So long as the theory is unable to assign values to the games typically found in application, only relatively simple situations—where games do not depend on other games—will be susceptible to analysis and solution. In the finite theory of von Neumann and Morgenstern difficulty in evaluation persists for the “essential” games, and for only those. In this note we deduce a value for the “essential” case and examine a number of its elementary properties. We proceed from a set of three axioms, having simple intuitive interpretations, which suffice to determine the value uniquely. Our present work, though mathematically self-contained, is founded conceptually on the von Neumann—Morgenstern theory up to their introduction of characteristic functions. We thereby inherit certain important underlying assumptions: (a) that utility is objective and transferable; (b) that games are cooperative affairs; (c) that games, granting (a) and (b), are adequately represented by their characteristic functions.}}, 
pages = {31--40}
}
@article{Shapley.1953, 
year = {1953}, 
title = {{Contributions to the Theory of Games (AM-28), Volume II}}, 
author = {Shapley, Lloyd S.}, 
doi = {10.1515/9781400881970-018}, 
pages = {307--318}, 
keywords = {}
}

@misc{Lundberg.2017,
  doi = {10.48550/ARXIV.1705.07874},
  
  url = {https://arxiv.org/abs/1705.07874},
  
  author = {Lundberg, Scott and Lee, Su-In},
  
  keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Unified Approach to Interpreting Model Predictions},
  
  publisher = {arXiv},
  
  year = {2017},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Covert.2020,
  doi = {10.48550/ARXIV.2011.14878},
  
  url = {https://arxiv.org/abs/2011.14878},
  
  author = {Covert, Ian and Lundberg, Scott and Lee, Su-In},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Explaining by Removing: A Unified Framework for Model Explanation},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{Covert.2020.SAGE,
  doi = {10.48550/ARXIV.2004.00668},
  
  url = {https://arxiv.org/abs/2004.00668},
  
  author = {Covert, Ian and Lundberg, Scott and Lee, Su-In},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Understanding Global Feature Contributions With Additive Importance Measures},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@unpublished{Lundberg.2019, 
year = {2019}, 
keywords = {Shapley,XAI}, 
author = {Scott, Lundberg,}, 
title = {{Consistent Individualized Feature Attribution for TreeEnsembles}}, 
url = {https://arxiv.org/pdf/1802.03888.pdf}, 
abstract = {{A game theoretic approach to explain the output of any machine learning      model. - slundberg/shap}}
}
@article{Hoepner.2020, 
year = {2020}, 
title = {{Significance, relevance and explainability in the machine learning age: an econometrics and financial data science perspective}}, 
author = {Hoepner, Andreas G. F. and McMillan, David and Vivian, Andrew and Simen, Chardin Wese}, 
journal = {The European Journal of Finance}, 
issn = {1351-847X}, 
doi = {10.1080/1351847x.2020.1847725}, 
abstract = {{Although machine learning is frequently associated with neural networks, it also comprises econometric regression approaches and other statistical techniques whose accuracy enhances with increasing observation. What constitutes high quality machine learning is yet unclear though. Proponents of deep learning (i.e. neural networks) value computational efficiency over human interpretability and tolerate the ‘black box’ appeal of their algorithms, whereas proponents of explainable artificial intelligence (xai) employ traceable ‘white box’ methods (e.g. regressions) to enhance explainability to human decision makers. We extend Brooks et al.’s [2019. ‘Financial Data Science: The Birth of a New Financial Research Paradigm Complementing Econometrics?’ European Journal of Finance 25 (17): 1627–36.] work on significance and relevance as assessment critieria in econometrics and financial data science to contribute to this debate. Specifically, we identify explainability as the Achilles heel of classic machine learning approaches such as neural networks, which are not fully replicable, lack transparency and traceability and therefore do not permit any attempts to establish causal inference. We conclude by suggesting routes for future research to advance the design and efficiency of ‘white box’ algorithms.}}, 
pages = {1--7}, 
number = {1-2}, 
volume = {27}, 
keywords = {}
}

@book{wooldridge2016,
  added-at = {2012-12-02T02:22:06.000+0100},
  author = {Wooldridge, Jeffrey Marc},
  biburl = {https://www.bibsonomy.org/bibtex/20f03974732e9a9b9f06cbd9cf65c71db/hackstutz},
  description = {Introductory Econometrics: A Modern Approach - Jeffrey M. Wooldridge - Google Books},
  interhash = {eef88a69347569c22d64f1622802e192},
  intrahash = {0f03974732e9a9b9f06cbd9cf65c71db},
  isbn = {9780324581621},
  keywords = {econometrics methoden statistik ökonometrie},
  lccn = {2007942361},
  publisher = {South-Western},
  series = {ISE - International Student Edition},
  timestamp = {2012-12-04T23:53:52.000+0100},
  title = {Introductory Econometrics: A Modern Approach},
  url = {http://books.google.ch/books?id=64vt5TDBNLwC},
  year = 2016
}

@article{friedman2001,
  title={Greedy function approximation: a gradient boosting machine},
  author={Friedman, Jerome H},
  journal={Annals of statistics},
  pages={1189--1232},
  year={2001},
  publisher={JSTOR}
}
@article{jensen2021a,
	title = {Is There a Replication Crisis in Finance?},
	author = {Jensen, Theis Ingerslev and Kelly, Bryan T. and Pedersen, Lasse Heje},
	year = {2021},
	date = {2021},
	journal = {SSRN Electronic Journal},
	doi = {10.2139/ssrn.3774514},
	url = {http://dx.doi.org/10.2139/ssrn.3774514},
	langid = {en}
}
@article{Samek.2021,
	doi = {10.1109/jproc.2021.3060483},
  
	url = {https://doi.org/10.1109%2Fjproc.2021.3060483},
  
	year = 2021,
	month = {mar},
  
	publisher = {Institute of Electrical and Electronics Engineers ({IEEE})},
  
	volume = {109},
  
	number = {3},
  
	pages = {247--278},
  
	author = {Wojciech Samek and Gregoire Montavon and Sebastian Lapuschkin and Christopher J. Anders and Klaus-Robert Muller},
  
	title = {Explaining Deep Neural Networks and Beyond: A Review of Methods and Applications},
  
	journal = {Proceedings of the {IEEE}
}
}
@book{Samek.2019,
  title={Explainable AI: interpreting, explaining and visualizing deep learning},
  author={Samek, Wojciech and Montavon, Gr{\'e}goire and Vedaldi, Andrea and Hansen, Lars Kai and M{\"u}ller, Klaus-Robert},
  volume={11700},
  year={2019},
  publisher={Springer Nature}
}

@article{Zhao.2021, 
year = {2021}, 
title = {{Causal Interpretations of Black-Box Models}}, 
author = {Zhao, Qingyuan and Hastie, Trevor}, 
journal = {Journal of Business \& Economic Statistics}, 
issn = {0735-0015}, 
doi = {10.1080/07350015.2019.1624293}, 
pmid = {33132490}, 
abstract = {{The fields of machine learning and causal inference have developed many concepts, tools, and theory that are potentially useful for each other. Through exploring the possibility of extracting causal interpretations from black-box machine-trained models, we briefly review the languages and concepts in causal inference that may be interesting to machine learning researchers. We start with the curious observation that Friedman’s partial dependence plot has exactly the same formula as Pearl’s back-door adjustment and discuss three requirements to make causal interpretations: a model with good predictive performance, some domain knowledge in the form of a causal diagram and suitable visualization tools. We provide several illustrative examples and find some interesting and potentially causal relations using visualization tools for black-box models.}}, 
pages = {1--19}, 
number = {1}, 
volume = {39}, 
keywords = {}
}


@article{goldstein2015,
	title = {Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation},
	author = {Goldstein, Alex and Kapelner, Adam and Bleich, Justin and Pitkin, Emil},
	year = {2015},
	month = {01},
	date = {2015-01-02},
	journal = {Journal of Computational and Graphical Statistics},
	pages = {44--65},
	volume = {24},
	number = {1},
	doi = {10.1080/10618600.2014.907095},
	url = {http://dx.doi.org/10.1080/10618600.2014.907095},
	langid = {en}
}

@article{Breiman.2001,
	author = {Breiman, Leo},
	year = {2001},
	date = {2001},
	journal = {Machine Learning},
	pages = {5--32},
	volume = {45},
	number = {1},
	doi = {10.1023/a:1010933404324},
	url = {http://dx.doi.org/10.1023/a:1010933404324}
}

@inbook{planning,
	title = {Planning Experiments},
	publisher = {Springer-Verlag},
	pages = {7--32},
	doi = {10.1007/0-387-22634-6_2},
	url = {http://dx.doi.org/10.1007/0-387-22634-6_2}
}

@article{holland1986,
	title = {Statistics and Causal Inference: Rejoinder},
	author = {Holland, Paul W.},
	year = {1986},
	month = {12},
	date = {1986-12},
	journal = {Journal of the American Statistical Association},
	pages = {968},
	volume = {81},
	number = {396},
	doi = {10.2307/2289069},
	url = {http://dx.doi.org/10.2307/2289069}
}

@article{pearl2009,
	title = {Causality},
	author = {Pearl, Judea},
	year = {2009},
	month = {09},
	date = {2009-09-14},
	doi = {10.1017/cbo9780511803161},
	url = {http://dx.doi.org/10.1017/CBO9780511803161}
}

@inbook{whatis2021,
	title = {What Is Causal Inference?},
	author = {Cunningham, Scott},
	year = {2021},
	month = {01},
	date = {2021-01-26},
	publisher = {Yale University Press},
	pages = {3--6},
	doi = {10.12987/9780300255881-003},
	url = {http://dx.doi.org/10.12987/9780300255881-003}
}

@techreport{imbens2019,
	title = {Potential Outcome and Directed Acyclic Graph Approaches to Causality: Relevance for Empirical Practice in Economics},
	author = {Imbens, Guido},
	year = {2019},
	month = {07},
	date = {2019-07},
	doi = {10.3386/w26104},
	url = {http://dx.doi.org/10.3386/w26104}
}

@inbook{danielsson2013,
	title = {Endogenous and Systemic Risk},
	author = {Danielsson, Jon and Shin, Hyun Song and Zigrand, Jean-Pierre},
	year = {2013},
	date = {2013},
	publisher = {University of Chicago Press},
	pages = {72--112},
	doi = {10.7208/chicago/9780226921969.003.0004},
	url = {http://dx.doi.org/10.7208/chicago/9780226921969.003.0004}
}

@article{heckman1979,
	title = {Sample Selection Bias as a Specification Error},
	author = {Heckman, James J.},
	year = {1979},
	month = {01},
	date = {1979-01},
	journal = {Econometrica},
	pages = {153},
	volume = {47},
	number = {1},
	doi = {10.2307/1912352},
	url = {http://dx.doi.org/10.2307/1912352}
}
